import boto3
import numpy as np
from PIL import Image
import io
import tensorflow as tf
from tensorflow.keras.models import load_model

# Initialize S3 client
s3_client = boto3.client('s3')

# Load your pre-trained model (make sure to upload the model to S3 or package it with Lambda)
model = load_model('/path/to/your/model.h5')  # Update this path

# Function to preprocess the image for prediction
def preprocess_image(image):
    image = image.resize((128, 128))  # Resize image
    image_array = np.array(image) / 255.0  # Normalize image
    return image_array.reshape(1, 128, 128, 3)  # Reshape for model input

# Function to classify images based on folder structure
def classify_images(bucket_name):
    # List all objects in the S3 bucket
    response = s3_client.list_objects_v2(Bucket=bucket_name)
    
    for obj in response.get('Contents', []):
        key = obj['Key']
        
        # Extract the folder name from the key
        folder_name = key.split('/')[0]  # Get the first part of the key as folder name
        
        # Retrieve the image from S3
        image_data = s3_client.get_object(Bucket=bucket_name, Key=key)['Body'].read()
        image = Image.open(io.BytesIO(image_data))
        
        # Process the image
        processed_image = preprocess_image(image)
        
        # Make prediction
        prediction = model.predict(processed_image)
        predicted_class_index = np.argmax(prediction, axis=1)[0]
        
        # Map the predicted class index to the folder name
        # Assuming you have a mapping of indices to folder names
        folder_names = ['Healthy', 'Diseased']  # Update this list based on your folder structure
        predicted_label = folder_names[predicted_class_index]
        
        # Output the folder name and predicted label
        print(f'Image: {key}, Folder Name: {folder_name}, Predicted Class: {predicted_label}')

# Call the function with your bucket name
classify_images('your-bucket-name')  # Replace with your actual bucket name
