# api/api_server.py
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request
from typing import List
from fastapi.responses import FileResponse
from pydantic import BaseModel
import uvicorn
import numpy as np
import tempfile
import shutil
import os
import uuid
import time
import zipfile
import logging
import threading
from pymongo import MongoClient
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from cnn_model.model import build_custom_cnn
from vector_db.milvus_utils import connect_milvus, create_collection, insert_embedding, search_embedding

# Logging setup
logging.basicConfig(filename='logs/error.log', level=logging.ERROR, format='%(asctime)s %(levelname)s:%(message)s')

# MongoDB setup
mongo_client = MongoClient("mongodb://localhost:27017/")
db = mongo_client["image_app"]
log_collection = db["logs"]

app = FastAPI()

# Load default model and setup Milvus
DEFAULT_MODEL_PATH = "cnn_model/weights/base_model.h5"
base_model = build_custom_cnn()
base_model.save(DEFAULT_MODEL_PATH)
feature_model = Model(inputs=base_model.input, outputs=base_model.get_layer("feature_vector").output)
connect_milvus()
create_collection()

# Utility to extract embedding from image
def extract_embedding_from_file(file_path, model):
    image = load_img(file_path, target_size=(640, 640))
    image_array = img_to_array(image) / 255.0
    image_array = np.expand_dims(image_array, axis=0)
    embedding_model = Model(inputs=model.input, outputs=model.get_layer("feature_vector").output)
    embedding = embedding_model.predict(image_array)[0]
    return embedding

@app.middleware("http")
async def log_exceptions(request: Request, call_next):
    try:
        return await call_next(request)
    except Exception as e:
        error_info = {
            "path": str(request.url),
            "error": str(e),
            "method": request.method,
            "timestamp": time.time()
        }
        logging.error(str(error_info))
        log_collection.insert_one(error_info)
        raise HTTPException(status_code=500, detail="Internal Server Error")

@app.post("/insert")
async def insert_image(file: UploadFile = File(...), model_version: str = Form("base")):
    if file.spool_max_size > 500 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="Image size exceeds 500MB limit")

    start_time = time.time()
    unique_id = str(uuid.uuid4())
    image_url = f"temp/{unique_id}.jpg"

    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
        shutil.copyfileobj(file.file, tmp)
        model_path = f"cnn_model/weights/{model_version}_model.h5"
        model = load_model(model_path)
        embedding = extract_embedding_from_file(tmp.name, model)
        insert_embedding(embedding)
        shutil.copy(tmp.name, image_url)

    elapsed_time = time.time() - start_time

    db.model_usage.update_one(
        {"model_version": model_version},
        {"$inc": {"usage_count": 1}, "$set": {"last_used": time.time()}},
        upsert=True
    )

    log_collection.insert_one({
        "event": "insert",
        "embedding_id": unique_id,
        "model_version": model_version,
        "image_url": image_url,
        "timestamp": time.time(),
        "source": "single"
    })

    return {
        "status": "success",
        "message": f"Image embedded using {model_version} model.",
        "embedding_id": unique_id,
        "image_url": image_url,
        "embedding_vector": embedding.tolist(),
        "processing_time_seconds": round(elapsed_time, 3)
    }

@app.post("/insert_batch_zip")
async def insert_batch_zip(zip_file: UploadFile = File(...), model_version: str = Form("base")):
    if zip_file.spool_max_size > 2 * 1024 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="ZIP size exceeds 2GB limit")

    temp_dir = tempfile.mkdtemp()
    zip_path = os.path.join(temp_dir, "uploaded.zip")

    with open(zip_path, "wb") as f:
        shutil.copyfileobj(zip_file.file, f)

    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    model_path = f"cnn_model/weights/{model_version}_model.h5"
    model = load_model(model_path)

    responses = []

    def process_file(fpath):
        start_time = time.time()
        unique_id = str(uuid.uuid4())
        image_url = f"temp/{unique_id}.jpg"
        embedding = extract_embedding_from_file(fpath, model)
        insert_embedding(embedding)
        shutil.copy(fpath, image_url)
        elapsed_time = time.time() - start_time
        responses.append({
            "embedding_id": unique_id,
            "image_url": image_url,
            "embedding_vector": embedding.tolist(),
            "processing_time_seconds": round(elapsed_time, 3)
        })

        log_collection.insert_one({
            "event": "insert",
            "embedding_id": unique_id,
            "model_version": model_version,
            "image_url": image_url,
            "timestamp": time.time(),
            "source": "batch"
        })

    threads = []
    for root, _, files in os.walk(temp_dir):
        for fname in files:
            if fname.lower().endswith((".jpg", ".jpeg", ".png")):
                fpath = os.path.join(root, fname)
                t = threading.Thread(target=process_file, args=(fpath,))
                threads.append(t)
                t.start()

    for t in threads:
        t.join()

    db.model_usage.update_one(
        {"model_version": model_version},
        {"$inc": {"usage_count": len(responses)}, "$set": {"last_used": time.time()}},
        upsert=True
    )

    return {"status": "batch_success", "count": len(responses), "results": responses}

@app.get("/download_model")
async def download_model(model_version: str = "base"):
    model_path = f"cnn_model/weights/{model_version}_model.h5"
    if not os.path.exists(model_path):
        return {"error": "Model version not found."}
    return FileResponse(model_path, filename=f"{model_version}_model.h5")

@app.get("/download_embeddings")
async def download_embeddings():
    meta_file = "temp/metadata_log.json"
    with open(meta_file, "w") as f:
        f.write("[\n  // Metadata logs could go here\n]")
    return FileResponse(meta_file, filename="metadata_log.json")

@app.post("/search")
async def search_image(file: UploadFile = File(...), top_k: int = 5, model_version: str = Form("base")):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
        shutil.copyfileobj(file.file, tmp)
        model_path = f"cnn_model/weights/{model_version}_model.h5"
        model = load_model(model_path)
        embedding = extract_embedding_from_file(tmp.name, model)
        results = search_embedding(embedding, top_k=top_k)

    matches = []
    for hits in results:
        for hit in hits:
            matches.append({"id": hit.id, "distance": hit.distance})

    return {
        "query_embedding": embedding.tolist(),
        "matches": matches
    }

@app.post("/train_model")
async def train_custom_model(train_data_path: str = Form(...), model_version: str = Form("custom")):
    import tensorflow as tf
    from tensorflow.keras.preprocessing.image import ImageDataGenerator

    model = build_custom_cnn()
    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
    train_gen = datagen.flow_from_directory(
        train_data_path,
        target_size=(640, 640),
        batch_size=4,
        class_mode='categorical',
        subset='training')

    val_gen = datagen.flow_from_directory(
        train_data_path,
        target_size=(640, 640),
        batch_size=4,
        class_mode='categorical',
        subset='validation')

    history = model.fit(train_gen, validation_data=val_gen, epochs=3)
    save_path = f"cnn_model/weights/{model_version}_model.h5"
    model.save(save_path)

    db.model_training.insert_one({
        "model_version": model_version,
        "dataset_path": train_data_path,
        "trained_at": time.time(),
        "epochs": 3,
        "samples": train_gen.samples,
        "metrics": {
            "loss": history.history.get("loss", []),
            "val_loss": history.history.get("val_loss", []),
            "accuracy": history.history.get("accuracy", []),
            "val_accuracy": history.history.get("val_accuracy", [])
        }
    })

    return {"status": "success", "message": f"Custom model '{model_version}' trained and saved."}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
