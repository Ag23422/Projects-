from warnings import filterwarnings
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import geopy as gps

import tensorflow as tf
from tensorflow import keras;keras.backend.clear_session()
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Accuracy
import matplotlib.pyplot as plt

API_KEY = "AkvYaDkG40h_rbg-DIGhsap7cxZic5v4GUlAWqeAMkZ1prAuXXx_TLRUGCrb-Uiv"  # Replace with your Bing Maps API key

geolocator = gps.geocoders.Bing('AkvYaDkG40h_rbg-DIGhsap7cxZic5v4GUlAWqeAMkZ1prAuXXx_TLRUGCrb-Uiv')



dataset = pd.read_csv('realtor-data.zip.csv')
dataset2= pd.read_csv('test.csv')

yt=input('if property is already selected and not in the given list.put the amount of the poperty :')
d=input('enter the preffered propetry amount: ')



j=dataset.iloc[:, :10]
o = dataset.iloc[:,1:].values
p = dataset.iloc[:, :7].values

#replace the X with the specific coloumn and add worl coordinates

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit(X[:, 0])
X[:, 0] = le.transform(X[:, 0])

le.fit(X[:, 1])
X[:, 1] = le.transform(X[:, 1])
  

  
le.fit(X[:, 3])
X[:, 3] = le.transform(X[:, 3])

  
le.fit(X[:, 4])
X[:, 4] = le.transform(X[:, 4])

le.fit(X[:, 10])
X[:, 10] = le.transform(X[:, 10])

le.fit(X[:, 2])
X[:, 2] = le.transform(X[:, 2])

for p in dataset.iloc[:,6:7].values:
  if (j<=o.all()):
    from sklearn.cluster import KMeans
    wcss = []
    for i in range(1, 11):
      kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
      kmeans.fit(o)
      kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
      m = kmeans.fit_predict(o)
      predictions = kmeans.predict(o)
      print(o[predictions == 0])


a = int(input("what is your monthley income "))
b= int(input("type of property required (enter the no of beds required)"))
c=int(input("what is your co-applicant's income(if any;if not type 0):"))
k=input('the loan amount required: ')
e=input('enter your gender: ')
g=input('enter your no of dependents : ')
f=input('no of working indivisuals : ')
q=input('tenure of loan amount(no of years) : ')

E=a+c
W={a,c,e,g,f}
print(W)
input("Enter 'yes' to confirm if above information is correct: ").lower() in ["yes", "y"]

X= dataset2.iloc[:, 1:].values


from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit(X[:, 0])
X[:, 0] = le.transform(X[:, 0])

le.fit(X[:, 1])
X[:, 1] = le.transform(X[:, 1])

le.fit(X[:, 2])
X[:, 2] = le.transform(X[:, 2])

le.fit(X[:, 3])
X[:, 3] = le.transform(X[:, 3])

le.fit(X[:, 4])
X[:, 4] = le.transform(X[:, 4])

le.fit(X[:, 10])
X[:, 10] = le.transform(X[:, 10])

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 1:])
X[:, 1:] = imputer.transform(X[:, 1:])


from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

X_train, X_test=  train_test_split(X,test_size=0.22)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
X_train.shape, X_test.shape

for W in dataset.iloc[:,1:].values:
  sc = StandardScaler()
  ann_model = keras.Sequential()

# adding dense layer
  ann_model.add(Dense(250, input_dim=13, kernel_initializer='he_normal', activation='relu'))
  ann_model.add(Dropout(0.2))
  ann_model.add(Dense(500, activation='relu'))
  ann_model.add(Dropout(0.2))
  ann_model.add(Dense(500, activation='relu'))
  ann_model.add(Dropout(0.2))
  ann_model.add(Dense(500, activation='relu'))
  ann_model.add(Dropout(0.4))
  ann_model.add(Dense(250, activation='linear'))
  ann_model.add(Dropout(0.4))

# adding dense layer with softmax activation/output layer
  ann_model.add(Dense(2, activation='softmax'))
  ann_model.summary()


  from keras import backend as K

  def recall_m(X_true, X_pred):
    true_positives = K.sum(K.round(K.clip(X_true * X_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(X_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

  def precision_m(X_true, X_pred):
    true_positives = K.sum(K.round(K.clip(X_true * X_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(X_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

  def f1_m(X_true, X_pred):
    precision = precision_m(X_true, X_pred)
    recall = recall_m(X_true, X_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))
  

  from sklearn.preprocessing import LabelEncoder
  le = LabelEncoder()
  le.fit(X[:, 0])
  X[:, 0] = le.transform(X[:, 0])

  le.fit(X[:, 1])
  X[:, 1] = le.transform(X[:, 1])
  

  
  le.fit(X[:, 3])
  X[:, 3] = le.transform(X[:, 3])

  
  le.fit(X[:, 4])
  X[:, 4] = le.transform(X[:, 4])

  le.fit(X[:, 10])
  X[:, 10] = le.transform(X[:, 10])

  le.fit(X[:, 2])
  X[:, 2] = le.transform(X[:, 2])
  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m])

  ann_model.fit(X_train,batch_size=100)

  X = np.array(X).astype('float32')
  predictions = tf.convert_to_tensor(X)
  predict = [X]

for i in predictions:
    predict.append(np.argmax(i))




