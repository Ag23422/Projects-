import numpy as np
import torch
import os
import cv2
from scipy.spatial.distance import cdist
from torch.utils.data import Dataset

class SparseDataset(Dataset):
    def __init__(self, root_dir, max_keypoints):
        self.nfeatures = max_keypoints
        self.sift = cv2.SIFT_create()
        self.matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)

        self.files = [
            os.path.join(subdir, fname)
            for subdir, _, filenames in os.walk(root_dir)
            for fname in filenames
            if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp'))
        ]

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_name = self.files[idx]

        # Load image (grayscale)
        image = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)
        if image is None:
            raise RuntimeError(f"Failed to load image {file_name}")

        # TODO: Replace this with your actual homography per sample if available
        M = np.eye(3, dtype=np.float32)  # identity for example

        # Warp image with homography M
        h, w = image.shape
        warped = cv2.warpPerspective(image, M, (w, h))

        # Detect SIFT keypoints and descriptors
        kp1, descs1 = self.sift.detectAndCompute(image, None)
        kp2, descs2 = self.sift.detectAndCompute(warped, None)

        # Limit number of keypoints to nfeatures
        kp1 = kp1[:self.nfeatures]
        kp2 = kp2[:self.nfeatures]
        descs1 = descs1[:len(kp1)]
        descs2 = descs2[:len(kp2)]

        # Convert keypoints to numpy arrays [N, 2]
        kp1_np = np.array([[kp.pt[0], kp.pt[1]] for kp in kp1], dtype=np.float32)
        kp2_np = np.array([[kp.pt[0], kp.pt[1]] for kp in kp2], dtype=np.float32)

        # Get keypoint responses as scores
        scores1_np = np.array([kp.response for kp in kp1], dtype=np.float32)
        scores2_np = np.array([kp.response for kp in kp2], dtype=np.float32)

        # Project keypoints1 using homography M
        kp1_projected = cv2.perspectiveTransform(kp1_np[None], M)[0]

        # Compute distances between projected kp1 and kp2
        dists = cdist(kp1_projected, kp2_np)

        # Find mutual nearest neighbors within 3 pixel threshold
        min1 = np.argmin(dists, axis=0)
        min2 = np.argmin(dists, axis=1)
        min1v = np.min(dists, axis=1)
        min1f = min2[min1v < 3]
        xx = np.where(min2[min1] == np.arange(len(min1)))[0]
        matches = np.intersect1d(min1f, xx)

        # Identify missing matches (keypoints with no match)
        missing1 = np.setdiff1d(np.arange(kp1_np.shape[0]), min1[matches])
        missing2 = np.setdiff1d(np.arange(kp2_np.shape[0]), matches)

        # Build matches matrix
        MN = np.stack([min1[matches], matches])
        MN2 = np.stack([missing1, np.full_like(missing1, len(kp2))])
        MN3 = np.stack([np.full_like(missing2, len(kp1)), missing2])
        all_matches = np.concatenate([MN, MN2, MN3], axis=1)

        # Convert images to tensors with shape [1, H, W] and scale [0,1]
        image_t = torch.from_numpy(image / 255.).unsqueeze(0).float()
        warped_t = torch.from_numpy(warped / 255.).unsqueeze(0).float()

        return {
            'keypoints0': torch.from_numpy(kp1_np[None]).float(),                # [1, N0, 2]
            'keypoints1': torch.from_numpy(kp2_np[None]).float(),                # [1, N1, 2]
            'descriptors0': torch.from_numpy((descs1.T / 256.).astype(np.float32)),  # [128, N0]
            'descriptors1': torch.from_numpy((descs2.T / 256.).astype(np.float32)),  # [128, N1]
            'scores0': torch.from_numpy(scores1_np),                             # [N0]
            'scores1': torch.from_numpy(scores2_np),                             # [N1]
            'image0': image_t,                                                    # [1, H, W]
            'image1': warped_t,                                                   # [1, H, W]
            'all_matches': torch.from_numpy(all_matches),                        # [2, N_matches]
            'file_name': os.path.basename(file_name),
            'skip_train': False
        }
