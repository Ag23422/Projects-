import cv2
import mediapipe as mp
import pyautogui
import numpy as np
from typing import Dict
from collections import deque

mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

cap = cv2.VideoCapture(0)

gestures: Dict[str, str] = {
    "fist": "pause",
    "palm": "play",

}

player = cv2.VideoCapture(0)

gesture_queue: deque = deque(maxlen=5)


def process_gesture(gesture: str):
    action = gestures.get(gesture)
    if action == "play":
        pyautogui.press('space')
    elif action == "pause":
        pyautogui.press('space')
    elif action == "rewind":
        pyautogui.hotkey('alt', 'left')
    elif action == "fast_forward":
        pyautogui.hotkey('alt', 'right')


while True:
    ret, frame = cap.read()

    frame = cv2.flip(frame, 1)

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles
    mp_hands = mp.solutions.hands

    results = hands.process(frame_rgb)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(
                frame,
                hand_landmarks,
                mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())
        # Extract the dominant hand landmarks
        landmarks = hand_landmarks.landmark
        x, y, z = int(landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * frame.shape[1]), int(
            landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * frame.shape[0]), landmarks[
            mp_hands.HandLandmark.INDEX_FINGER_TIP].z
        # Identify the gesture based on the landmarks
        if x > frame.shape[1] * 0.75 and y > frame.shape[0] * 0.75:
            gesture = "fist"
        elif x < frame.shape[1] * 0.25 and y > frame.shape[0] * 0.75:
            gesture = "thumb"
        elif x > frame.shape[1] * 0.5 and y < frame.shape[0] * 0.25:
            gesture = "palm"
        else:
            gesture = None
        # Add the recognized gesture to the queue
        gesture_queue.append(gesture)
        # Process the gestures in the queue
        for gesture in gesture_queue:
            if gesture:
                process_gesture(gesture)

    cv2.imshow('Hand Gesture Control', frame)

    if cv2.waitKey(5) & 0xFF == ord('q'):
        break

cap.release()
player.release()
cv2.destroyAllWindows()
